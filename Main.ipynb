{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import kendalltau\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133 5451\n"
     ]
    }
   ],
   "source": [
    "# Load the .mat file\n",
    "mat_file = sio.loadmat('E:/M Tech/3. Web & Social Computing/project/code/data/05_Email.mat')\n",
    "\n",
    "# Get the adjacency matrix\n",
    "\n",
    "adj_matrix = mat_file['Email']\n",
    "# adj_matrix = mat_file['polblogs']\n",
    "# adj_matrix = mat_file['Router5022']\n",
    "# adj_matrix = mat_file['facebook4039']\n",
    "\n",
    "# Create a graph from the adjacency matrix\n",
    "G = nx.from_numpy_array(adj_matrix)\n",
    "print(nx.number_of_nodes(G), nx.number_of_edges(G))\n",
    "\n",
    "# gravity radious\n",
    "R = 3\n",
    "infect_prob = 0.30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.62224183583407"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average degree of the network\n",
    "avg_degree = sum(dict(G.degree()).values()) / len(G)\n",
    "avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.606032017315423"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average distance of the network\n",
    "avg_distance = nx.average_shortest_path_length(G)\n",
    "avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Degree Centrality ##########\n",
    "deg_centrality = nx.degree_centrality(G)\n",
    "\n",
    "list_degree = []\n",
    "rank_degree = []\n",
    "# print the degree centrality for each node\n",
    "for node, centrality in sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True):\n",
    "    list_degree.append(node)\n",
    "    rank_degree.append((node, centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Betweenness Centrality ##########\n",
    "btn_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "list_btn = []\n",
    "rank_btn = []\n",
    "# print the betweenness centrality for each node\n",
    "for node, centrality in sorted(btn_centrality.items(), key=lambda x: x[1], reverse=True):\n",
    "    list_btn.append(node)\n",
    "    rank_btn.append((node, centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Closeness Centrality ##########\n",
    "cc_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "list_cc = []\n",
    "rank_cc = []\n",
    "# print the closeness centrality for each node\n",
    "for node, centrality in sorted(cc_centrality.items(), key=lambda x: x[1], reverse=True):\n",
    "    list_cc.append(node)\n",
    "    rank_cc.append((node, centrality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k-shell of the graph\n",
    "def k_shell_ranking(G):\n",
    "    kcores = nx.core_number(G) # calculate core number for each node\n",
    "    k_max = max(kcores.values()) # find maximum core number\n",
    "\n",
    "    # initialize dictionary to store k-shell centrality values\n",
    "    k_shell = dict.fromkeys(G.nodes(), 0)\n",
    "\n",
    "    # iterate over each k-shell\n",
    "    for k in range(k_max+1):\n",
    "        shell_nodes = [node for node in G.nodes() if kcores[node] >= k]\n",
    "\n",
    "        # if there are no nodes in the k-shell, exit loop\n",
    "        if len(shell_nodes) == 0:\n",
    "            break\n",
    "\n",
    "        # remove nodes in the k-shell and update k-shell centrality values\n",
    "        G_k = G.subgraph(shell_nodes)\n",
    "        k_shell_k = nx.core_number(G_k)\n",
    "        for node in shell_nodes:\n",
    "            k_shell[node] = k_shell_k[node] + k\n",
    "\n",
    "    ranking = sorted(k_shell.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranking\n",
    "\n",
    "# call the kshell method \n",
    "rank_kshell = k_shell_ranking(G)\n",
    "list_kshell = [i[0] for i in rank_kshell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entropy of nodes\n",
    "def EnRenewRank(G, topk, order):\n",
    "    # N - 1\n",
    "    all_degree = nx.number_of_nodes(G) - 1\n",
    "    # avg degree\n",
    "    k_ = nx.number_of_edges(G) * 2 / nx.number_of_nodes(G)\n",
    "    # E<k>\n",
    "    k_entropy = - k_ * ((k_ / all_degree) * math.log((k_ / all_degree)))\n",
    "\n",
    "    # node's information pi\n",
    "    node_information = {}\n",
    "    for node in nx.nodes(G):\n",
    "        information = (G.degree(node) / all_degree)\n",
    "        try:\n",
    "            node_information[node] = - information * math.log(information)\n",
    "        except:\n",
    "            node_information[node] = 0\n",
    "    # node's entropy Ei\n",
    "    node_entropy = {}\n",
    "    for node in nx.nodes(G):\n",
    "        node_entropy[node] = 0\n",
    "        for nbr in nx.neighbors(G, node):\n",
    "            node_entropy[node] += node_information[nbr]\n",
    "\n",
    "    rank = []\n",
    "    for i in range(topk):\n",
    "        # choose the max entropy node\n",
    "        max_entropy_node, entropy = max(node_entropy.items(), key=lambda x: x[1])\n",
    "        rank.append((max_entropy_node, entropy))\n",
    "\n",
    "        cur_nbrs = nx.neighbors(G, max_entropy_node)\n",
    "        for o in range(order):\n",
    "            for nbr in cur_nbrs:\n",
    "                if nbr in node_entropy:\n",
    "                        node_entropy[nbr] -= (node_information[max_entropy_node] / k_entropy) / (2**o)\n",
    "            next_nbrs = []\n",
    "            for node in cur_nbrs:\n",
    "                nbrs = nx.neighbors(G, node)\n",
    "                next_nbrs.extend(nbrs)\n",
    "            cur_nbrs = next_nbrs\n",
    "\n",
    "        #set the information quantity of selected nodes to 0\n",
    "        node_information[max_entropy_node] = 0\n",
    "        # set entropy to 0\n",
    "        node_entropy.pop(max_entropy_node)\n",
    "    return rank\n",
    "\n",
    "# find entropy for all nodes.\n",
    "rank_entropy = EnRenewRank(G, nx.number_of_nodes(G), 2)\n",
    "list_entropy = [i[0] for i in rank_entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sir_value(G, node, avg, infect_prob, cover_prob, max_iter):\n",
    "\n",
    "    time_num_dict_list = []\n",
    "    time_list = []\n",
    "\n",
    "    for i in range(avg):\n",
    "        time, time_num_dict = SIR(G, node.copy(), infect_prob, cover_prob, max_iter)\n",
    "        time_num_dict_list.append((list(time_num_dict.values())))\n",
    "        time_list.append(time)\n",
    "            \n",
    "    max_time = max(time_list) + 1\n",
    "    result_matrix = np.zeros((len(time_num_dict_list), max_time))\n",
    "    for index, (row, time_num_dict) in enumerate(zip(result_matrix, time_num_dict_list)):\n",
    "        row[:] = time_num_dict[-1]\n",
    "        row[0:len(time_num_dict)] = time_num_dict\n",
    "        result_matrix[index] = row\n",
    "    return np.mean(result_matrix, axis=0)\n",
    "\n",
    "\n",
    "def SIR(g, infeacted_set, infect_prob, cover_prob, max_iter):\n",
    "\n",
    "    time = 0\n",
    "    time_count_dict = {}\n",
    "    time_count_dict[time] = len(infeacted_set)\n",
    "    totalNodes = nx.number_of_nodes(g)\n",
    "    # infeacted_set = infeacted_set\n",
    "    node_state = {}\n",
    "    covered_set = set()\n",
    "\n",
    "    for node in nx.nodes(g):\n",
    "        if node in infeacted_set:\n",
    "            node_state[node] = 'i'\n",
    "        else:\n",
    "            node_state[node] = 's'\n",
    "\n",
    "    while len(infeacted_set) != 0 and max_iter != 0:\n",
    "        ready_to_cover = []\n",
    "        ready_to_infeact = []\n",
    "        for node in infeacted_set:\n",
    "            nbrs = nx.neighbors(g, node)\n",
    "            for singleNode in list(nbrs):\n",
    "                if random.uniform(0, 1) <= infect_prob and node_state[singleNode] == 's':\n",
    "                    node_state[singleNode] = 'i'\n",
    "                    ready_to_infeact.append(singleNode)\n",
    "            if random.uniform(0, 1) <= cover_prob:\n",
    "                ready_to_cover.append(node)\n",
    "        for node in ready_to_cover:\n",
    "            node_state[node] = 'r'\n",
    "            infeacted_set.remove(node)\n",
    "            covered_set.add(node)\n",
    "        for node in ready_to_infeact:\n",
    "            infeacted_set.append(node)\n",
    "        max_iter -= 1\n",
    "        time += 1\n",
    "        time_count_dict[time] = len(covered_set) / totalNodes\n",
    "    return time, time_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_SIR = []\n",
    "\n",
    "for i in list(G.nodes()):\n",
    "    rank_SIR.append([i,get_sir_value(G, [i], 100, infect_prob, 1, 1000)[-1]])\n",
    "rank_SIR = sorted(rank_SIR, key = lambda x: x[1], reverse=True)\n",
    "list_SIR = [i[0] for i in rank_SIR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the shortest paths between all pairs of nodes\n",
    "all_shortest_paths = dict(nx.all_pairs_shortest_path_length(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the influence of each node\n",
    "# Using modified Base Gravity model (k-shell + entropy)\n",
    "influence_scores = {}\n",
    "for node in G.nodes():\n",
    "    influence_score = 0\n",
    "    DKi = dict(rank_entropy)[node] + dict(rank_kshell)[node]\n",
    "    for other_node in G.nodes():\n",
    "        distance = all_shortest_paths[node][other_node]\n",
    "        if node != other_node and R >= distance:\n",
    "            DKj = dict(rank_entropy)[other_node] + dict(rank_kshell)[other_node]\n",
    "            strength = DKi * DKj\n",
    "            influence_score += strength / (distance ** 2)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# Rank the nodes based on their influence scores\n",
    "\n",
    "sorted_nodes = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "rank_gravity_entropy = []\n",
    "list_gravity_entropy = []\n",
    "for node, score in sorted_nodes:\n",
    "    rank_gravity_entropy.append((node, score))\n",
    "    list_gravity_entropy.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the influence of each node\n",
    "# implemented by base Gravity model (k-shell + degree)\n",
    "influence_scores = {}\n",
    "for node in G.nodes():\n",
    "    influence_score = 0\n",
    "    DKi = dict(rank_degree)[node] + dict(rank_kshell)[node]\n",
    "    for other_node in G.nodes():\n",
    "        distance = all_shortest_paths[node][other_node]\n",
    "        if node != other_node and R >= distance:\n",
    "            DKj = dict(rank_degree)[other_node] + dict(rank_kshell)[other_node]\n",
    "            strength = DKi * DKj\n",
    "            influence_score += strength / (distance ** 2)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# Rank the nodes based on their influence scores\n",
    "\n",
    "sorted_nodes = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "rank_gravity_base = []\n",
    "list_gravity_base = []\n",
    "for node, score in sorted_nodes:\n",
    "    rank_gravity_base.append((node, score))\n",
    "    list_gravity_base.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the influence of each node\n",
    "# implemented by GC aglorithm (only k-shell)\n",
    "influence_scores = {}\n",
    "for node in G.nodes():\n",
    "    influence_score = 0\n",
    "    DKi = dict(rank_kshell)[node]\n",
    "    for other_node in G.nodes():\n",
    "        distance = all_shortest_paths[node][other_node]\n",
    "        if node != other_node and R >= distance:\n",
    "            DKj = dict(rank_kshell)[other_node]\n",
    "            strength = DKi * DKj\n",
    "            influence_score += strength / (distance ** 2)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# Rank the nodes based on their influence scores\n",
    "\n",
    "sorted_nodes = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "rank_GC = []\n",
    "list_GC = []\n",
    "for node, score in sorted_nodes:\n",
    "    rank_GC.append((node, score))\n",
    "    list_GC.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the influence of each node\n",
    "# implemented by LGM aglorithm (only degree)\n",
    "influence_scores = {}\n",
    "for node in G.nodes():\n",
    "    influence_score = 0\n",
    "    DKi = dict(rank_degree)[node]\n",
    "    for other_node in G.nodes():\n",
    "        distance = all_shortest_paths[node][other_node]\n",
    "        if node != other_node and R >= distance:\n",
    "            DKj = dict(rank_degree)[other_node]\n",
    "            strength = DKi * DKj\n",
    "            influence_score += strength / (distance ** 2)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# Rank the nodes based on their influence scores\n",
    "\n",
    "sorted_nodes = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "rank_LGM = []\n",
    "list_LGM = []\n",
    "for node, score in sorted_nodes:\n",
    "    rank_LGM.append((node, score))\n",
    "    list_LGM.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnTau(rankList):\n",
    "    tau, p_value = kendalltau(list_SIR, rankList)\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonicity(G,rank_list):\n",
    "  dist = {}\n",
    "  num = nx.number_of_nodes(G)\n",
    "\n",
    "\n",
    "  for node,val in rank_list:\n",
    "    if val in dist:\n",
    "      dist[val] += 1\n",
    "    else :\n",
    "      dist[val] = 1\n",
    "\n",
    "  sum = 0\n",
    "\n",
    "  for value in dist.values():\n",
    "      sum += (value*(value - 1))\n",
    "\n",
    "  m = sum/(num*(num-1))\n",
    "  monotonicity = (1 - m)**2\n",
    "\n",
    "  return monotonicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC : 0.2510019055698153\n",
      "Betweenness : 0.24152395684866781\n",
      "Clossness : 0.2876786666625083\n",
      "K-shell : 0.2754312482261983\n",
      "Entropy : -0.07191264942817312\n",
      "New Gravity : 0.2708840783560328\n",
      "Base Gravity : 0.28203680774952516\n",
      "GC : 0.2861192805616285\n",
      "LGM : 0.29279345307339405\n"
     ]
    }
   ],
   "source": [
    "# find the Kendall's tau\n",
    "listOfRanks = {\"DC\" : list_degree, \"Betweenness\":list_btn, \"Clossness\":list_cc, \"K-shell\":list_kshell, \"Entropy\":list_entropy, \n",
    "               \"New Gravity\":list_gravity_entropy, \"Base Gravity\":list_gravity_base, \"GC\":list_GC, \"LGM\":list_LGM}\n",
    "for rankMethod in listOfRanks:\n",
    "    print(rankMethod + \" : \" + str(returnTau(listOfRanks[rankMethod])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC : 0.8873673021840296\n",
      "Betweenness : 0.9400088030354118\n",
      "Clossness : 0.9988027500410694\n",
      "K-shell : 0.8088125575542995\n",
      "Entropy : 0.9994075210654945\n",
      "New Gravity : 0.9999033203985946\n",
      "Base Gravity : 0.9998970831654469\n",
      "GC : 0.999893964556168\n",
      "LGM : 0.9998970831654469\n"
     ]
    }
   ],
   "source": [
    "rankOfMethos = {\"DC\" : rank_degree, \"Betweenness\":rank_btn, \"Clossness\":rank_cc, \"K-shell\":rank_kshell, \"Entropy\":rank_entropy, \n",
    "               \"New Gravity\":rank_gravity_entropy, \"Base Gravity\":rank_gravity_base, \"GC\":rank_GC, \"LGM\":rank_LGM}\n",
    "\n",
    "for rankName in rankOfMethos:\n",
    "    print(rankName + \" : \" + str(monotonicity(G, rankOfMethos[rankName])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
